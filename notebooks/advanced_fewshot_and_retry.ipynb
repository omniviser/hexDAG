{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Advanced Patterns: Few-Shot Learning with Intelligent Retry\n",
    "\n",
    "This notebook demonstrates **production-ready AI patterns**:\n",
    "\n",
    "1. **Few-Shot Learning** - Train with examples in the prompt\n",
    "2. **Structured Output** - Type-safe JSON parsing with Pydantic\n",
    "3. **Intelligent Retry** - Auto-correction when LLM returns invalid JSON\n",
    "4. **Error Context** - Parser provides hints for retry prompts\n",
    "5. **Unified LLM Node** - Single node handles prompting, LLM calls, and parsing\n",
    "\n",
    "## Real-World Use Case: Product Review Sentiment Analysis\n",
    "\n",
    "We'll build a system that:\n",
    "- Classifies sentiment with **confidence scores**\n",
    "- Extracts **key aspects** (quality, price, shipping)\n",
    "- Handles **parsing failures** gracefully\n",
    "- Uses **few-shot examples** for better accuracy\n",
    "\n",
    "## Architecture\n",
    "\n",
    "**Using LLMNode** (unified approach):\n",
    "```\n",
    "LLMNode (prompt_template + parse_json + output_schema)\n",
    "    ‚Üì\n",
    "Structured Output (Pydantic validated)\n",
    "```\n",
    "\n",
    "The `llm_node` combines prompt templating, LLM API calls, and JSON parsing in a single node for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from hexdag.core.orchestration.orchestrator import Orchestrator\n",
    "from hexdag.core.pipeline_builder import YamlPipelineBuilder\n",
    "from hexdag.core.pipeline_builder.component_instantiator import ComponentInstantiator\n",
    "\n",
    "# Helper for port instantiation\n",
    "instantiator = ComponentInstantiator()\n",
    "\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Example 1: Few-Shot Sentiment Classification\n",
    "\n",
    "Using **few-shot learning** - the LLM learns from examples in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-Shot Sentiment Classifier with Structured Output\n",
    "# Using llm_node which combines prompt templating, LLM calls, and parsing\n",
    "fewshot_pipeline = \"\"\"\n",
    "apiVersion: hexdag/v1\n",
    "kind: Pipeline\n",
    "metadata:\n",
    "  name: fewshot-sentiment-classifier\n",
    "  description: Production sentiment analysis with few-shot learning\n",
    "\n",
    "spec:\n",
    "  ports:\n",
    "    llm:\n",
    "      adapter: hexdag.builtin.adapters.mock.MockLLM\n",
    "\n",
    "  nodes:\n",
    "    # Single llm_node handles prompt building, LLM call, and JSON parsing\n",
    "    - kind: llm_node\n",
    "      metadata:\n",
    "        name: parse_sentiment\n",
    "      spec:\n",
    "        prompt_template: |\n",
    "          You are a product review sentiment analyzer. Classify reviews and extract key aspects.\n",
    "          \n",
    "          ## Few-Shot Examples:\n",
    "          \n",
    "          Review: \"Terrible quality, broke after 2 days. Complete waste of money!\"\n",
    "          Output: {\"sentiment\": \"negative\", \"confidence\": 0.95, \"aspects\": {\"quality\": \"poor\", \"value\": \"poor\", \"durability\": \"poor\"}, \"key_phrases\": [\"terrible quality\", \"broke after 2 days\", \"waste of money\"]}\n",
    "          \n",
    "          Review: \"Decent product for the price. Works as expected, nothing special.\"\n",
    "          Output: {\"sentiment\": \"neutral\", \"confidence\": 0.78, \"aspects\": {\"quality\": \"acceptable\", \"value\": \"good\", \"performance\": \"acceptable\"}, \"key_phrases\": [\"decent\", \"works as expected\", \"nothing special\"]}\n",
    "          \n",
    "          Review: \"Amazing! Best purchase ever. Great quality and fast shipping!\"\n",
    "          Output: {\"sentiment\": \"positive\", \"confidence\": 0.92, \"aspects\": {\"quality\": \"excellent\", \"value\": \"excellent\", \"shipping\": \"fast\"}, \"key_phrases\": [\"amazing\", \"best purchase\", \"great quality\"]}\n",
    "          \n",
    "          ## Now classify this review:\n",
    "          \n",
    "          Review: \"{{review}}\"\n",
    "          \n",
    "          Output (JSON only, no explanation):\n",
    "        parse_json: true\n",
    "        output_schema:\n",
    "          sentiment: str\n",
    "          confidence: float\n",
    "          aspects: dict\n",
    "          key_phrases: list\n",
    "\"\"\"\n",
    "\n",
    "builder = YamlPipelineBuilder()\n",
    "graph_fewshot, config_fewshot = builder.build_from_yaml_string(fewshot_pipeline)\n",
    "\n",
    "print(f\"‚úÖ Few-Shot Pipeline: {len(graph_fewshot.nodes)} nodes\")\n",
    "print(f\"üìã Nodes: {list(graph_fewshot.nodes.keys())}\")\n",
    "print(\"üìù Template includes 3 few-shot examples\")\n",
    "print(\"üéØ Output schema: sentiment, confidence, aspects, key_phrases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure mock LLM with realistic response\n",
    "from hexdag.builtin.adapters.mock.mock_llm import MockLLM\n",
    "\n",
    "# Create mock with a realistic sentiment analysis response\n",
    "mock_response = \"\"\"{\n",
    "  \"sentiment\": \"positive\",\n",
    "  \"confidence\": 0.89,\n",
    "  \"aspects\": {\n",
    "    \"quality\": \"excellent\",\n",
    "    \"value\": \"good\",\n",
    "    \"usability\": \"excellent\"\n",
    "  },\n",
    "  \"key_phrases\": [\"love this product\", \"works perfectly\", \"great value\"]\n",
    "}\"\"\"\n",
    "\n",
    "mock_llm = MockLLM(responses=[mock_response])\n",
    "\n",
    "# Instantiate ports with our custom mock\n",
    "ports_fewshot = {\"llm\": mock_llm}\n",
    "orchestrator_fewshot = Orchestrator(ports=ports_fewshot)\n",
    "\n",
    "# Test with a real review\n",
    "result = await orchestrator_fewshot.run(\n",
    "    graph_fewshot,\n",
    "    {\"review\": \"I absolutely love this product! Works perfectly and great value for money.\"},\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Few-Shot Classification Results:\")\n",
    "parsed = result[\"parse_sentiment\"]\n",
    "# Handle both dict and object results\n",
    "if isinstance(parsed, dict):\n",
    "    print(f\"Sentiment: {parsed.get('sentiment')}\")\n",
    "    print(f\"Confidence: {parsed.get('confidence')}\")\n",
    "    print(\"\\nAspects:\")\n",
    "    for aspect, rating in parsed.get(\"aspects\", {}).items():\n",
    "        print(f\"  ‚Ä¢ {aspect}: {rating}\")\n",
    "    print(\"\\nKey Phrases:\")\n",
    "    for phrase in parsed.get(\"key_phrases\", []):\n",
    "        print(f\"  ‚Ä¢ {phrase}\")\n",
    "else:\n",
    "    print(f\"Sentiment: {parsed.sentiment}\")\n",
    "    print(f\"Confidence: {parsed.confidence}\")\n",
    "    print(\"\\nAspects:\")\n",
    "    for aspect, rating in parsed.aspects.items():\n",
    "        print(f\"  ‚Ä¢ {aspect}: {rating}\")\n",
    "    print(\"\\nKey Phrases:\")\n",
    "    for phrase in parsed.key_phrases:\n",
    "        print(f\"  ‚Ä¢ {phrase}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Example 2: Handling Parse Errors\n",
    "\n",
    "What happens when the LLM returns **invalid JSON**? Let's demonstrate error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate LLM returning INVALID JSON\n",
    "bad_responses = [\n",
    "    # Response 1: Invalid JSON (missing closing brace)\n",
    "    \"\"\"{\n",
    "  \"sentiment\": \"positive\",\n",
    "  \"confidence\": 0.85,\n",
    "  \"aspects\": {\n",
    "    \"quality\": \"good\"\n",
    "  \"\"\",\n",
    "    # Response 2: Valid JSON but missing required field\n",
    "    \"\"\"{\n",
    "  \"sentiment\": \"positive\",\n",
    "  \"confidence\": 0.85\n",
    "}\"\"\",\n",
    "    # Response 3: LLM adds explanation text\n",
    "    \"\"\"Here's my analysis:\n",
    "{\n",
    "  \"sentiment\": \"positive\",\n",
    "  \"confidence\": 0.85,\n",
    "  \"aspects\": {\"quality\": \"good\"},\n",
    "  \"key_phrases\": [\"great\"]\n",
    "}\n",
    "I think this review is positive because...\"\"\",\n",
    "]\n",
    "\n",
    "# Test with invalid JSON (should fail gracefully)\n",
    "mock_llm_bad = MockLLM(responses=[bad_responses[0]])\n",
    "ports_bad = {\"llm\": mock_llm_bad}\n",
    "orchestrator_bad = Orchestrator(ports=ports_bad)\n",
    "\n",
    "print(\"üî¥ Testing with INVALID JSON (missing closing brace)...\\n\")\n",
    "try:\n",
    "    result_bad = await orchestrator_bad.run(graph_fewshot, {\"review\": \"Test review\"})\n",
    "    print(\"‚úÖ Parsing succeeded (unexpected!)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Parse Error (expected): {type(e).__name__}\")\n",
    "    print(f\"   Message: {str(e)[:200]}...\")\n",
    "    print(\"\\nüí° In production, this would trigger a RETRY with error correction prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Example 3: JSON-in-Markdown Strategy\n",
    "\n",
    "Handle LLMs that wrap JSON in markdown code blocks using the **`json_in_markdown`** parser strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline with json_in_markdown parser strategy\n",
    "# Using llm_node with parse_strategy parameter\n",
    "markdown_pipeline = \"\"\"\n",
    "apiVersion: hexdag/v1\n",
    "kind: Pipeline\n",
    "metadata:\n",
    "  name: markdown-aware-classifier\n",
    "\n",
    "spec:\n",
    "  ports:\n",
    "    llm:\n",
    "      adapter: hexdag.builtin.adapters.mock.MockLLM\n",
    "\n",
    "  nodes:\n",
    "    # Single llm_node with json_in_markdown strategy handles JSON in code blocks\n",
    "    - kind: llm_node\n",
    "      metadata:\n",
    "        name: parse_result\n",
    "      spec:\n",
    "        prompt_template: |\n",
    "          Classify this review: {{review}}\n",
    "          \n",
    "          Return JSON with: sentiment, confidence, key_phrases\n",
    "        parse_json: true\n",
    "        parse_strategy: json_in_markdown\n",
    "        output_schema:\n",
    "          sentiment: str\n",
    "          confidence: float\n",
    "          key_phrases: list\n",
    "\"\"\"\n",
    "\n",
    "graph_markdown, config_markdown = builder.build_from_yaml_string(markdown_pipeline)\n",
    "\n",
    "# Mock LLM that returns JSON wrapped in markdown\n",
    "markdown_response = \"\"\"```json\n",
    "{\n",
    "  \"sentiment\": \"positive\",\n",
    "  \"confidence\": 0.91,\n",
    "  \"key_phrases\": [\"excellent product\", \"highly recommend\", \"great service\"]\n",
    "}\n",
    "```\"\"\"\n",
    "\n",
    "mock_markdown = MockLLM(responses=[markdown_response])\n",
    "ports_markdown = {\"llm\": mock_markdown}\n",
    "orchestrator_markdown = Orchestrator(ports=ports_markdown)\n",
    "\n",
    "result_markdown = await orchestrator_markdown.run(\n",
    "    graph_markdown, {\"review\": \"Excellent product! Highly recommend. Great service too.\"}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Extracted JSON from markdown code block!\")\n",
    "parsed_md = result_markdown[\"parse_result\"]\n",
    "# Handle both dict and object results\n",
    "if isinstance(parsed_md, dict):\n",
    "    print(f\"\\nSentiment: {parsed_md.get('sentiment')}\")\n",
    "    print(f\"Confidence: {parsed_md.get('confidence')}\")\n",
    "    print(f\"Key Phrases: {', '.join(parsed_md.get('key_phrases', []))}\")\n",
    "else:\n",
    "    print(f\"\\nSentiment: {parsed_md.sentiment}\")\n",
    "    print(f\"Confidence: {parsed_md.confidence}\")\n",
    "    print(f\"Key Phrases: {', '.join(parsed_md.key_phrases)}\")\n",
    "print(\"\\nüí° The parser automatically extracted JSON from the ```json code block\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Summary: Production-Ready Patterns\n",
    "\n",
    "This notebook demonstrated **real-world AI engineering patterns**:\n",
    "\n",
    "### ‚úÖ What We Built\n",
    "\n",
    "1. **Few-Shot Learning**\n",
    "   - 3 labeled examples in the prompt\n",
    "   - LLM learns classification patterns\n",
    "   - Structured output with confidence scores\n",
    "\n",
    "2. **Structured Output Parsing**\n",
    "   - Type-safe schemas with Pydantic\n",
    "   - Multiple fields: sentiment, confidence, aspects, key_phrases\n",
    "   - Automatic validation\n",
    "\n",
    "3. **Error Handling**\n",
    "   - Demonstrated parse failures (invalid JSON)\n",
    "   - Showed different parser strategies (`json`, `json_in_markdown`)\n",
    "   - Clear error messages for debugging\n",
    "\n",
    "4. **Batch Processing**\n",
    "   - Processed 5 reviews efficiently\n",
    "   - Aggregated statistics\n",
    "   - Production-ready workflow\n",
    "\n",
    "### üéØ Key Takeaways\n",
    "\n",
    "**Composable Architecture Benefits**:\n",
    "- **Separation of Concerns**: Prompting ‚â† LLM calls ‚â† Parsing\n",
    "- **Testability**: Each component independently testable\n",
    "- **Flexibility**: Swap prompts, parsers, or LLMs without changing code\n",
    "- **Type Safety**: Pydantic validation catches errors early\n",
    "\n",
    "**Few-Shot Learning**:\n",
    "- 3-5 examples dramatically improve accuracy\n",
    "- Examples teach the LLM your exact output format\n",
    "- More reliable than zero-shot classification\n",
    "\n",
    "**Error Handling**:\n",
    "- Parser provides helpful error messages\n",
    "- Multiple parsing strategies for different LLM behaviors\n",
    "- Future: Automatic retry with error correction prompts\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "**For Production**:\n",
    "1. Add **retry policies** with exponential backoff\n",
    "2. Implement **error correction prompts** (pass parse errors back to LLM)\n",
    "3. Use **real LLM** (`core:openai` instead of `plugin:mock`)\n",
    "4. Add **observability** (logging, metrics, tracing)\n",
    "5. Implement **caching** for repeated queries\n",
    "\n",
    "**Advanced Patterns**:\n",
    "- **Chain-of-Thought**: Ask LLM to explain reasoning\n",
    "- **Self-Consistency**: Run multiple times, vote on results\n",
    "- **Active Learning**: Flag low-confidence for human review\n",
    "- **Dynamic Few-Shot**: Select best examples based on input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch process multiple reviews\n",
    "reviews_batch = [\n",
    "    \"Amazing product! Works exactly as advertised. Will buy again!\",\n",
    "    \"Terrible experience. Product arrived damaged and customer service was unhelpful.\",\n",
    "    \"It's okay. Does the job but nothing special. Overpriced for what you get.\",\n",
    "    \"Love it! Great quality and fast delivery. Exceeded my expectations!\",\n",
    "    \"Disappointed. Doesn't work as described. Requesting a refund.\",\n",
    "]\n",
    "\n",
    "# Create mock with multiple responses\n",
    "batch_responses = [\n",
    "    '{\"sentiment\": \"positive\", \"confidence\": 0.94, \"aspects\": {\"quality\": \"excellent\", \"value\": \"good\"}, \"key_phrases\": [\"amazing\", \"works as advertised\", \"will buy again\"]}',\n",
    "    '{\"sentiment\": \"negative\", \"confidence\": 0.96, \"aspects\": {\"quality\": \"poor\", \"service\": \"poor\"}, \"key_phrases\": [\"terrible\", \"damaged\", \"unhelpful\"]}',\n",
    "    '{\"sentiment\": \"neutral\", \"confidence\": 0.82, \"aspects\": {\"quality\": \"acceptable\", \"value\": \"poor\"}, \"key_phrases\": [\"okay\", \"does the job\", \"overpriced\"]}',\n",
    "    '{\"sentiment\": \"positive\", \"confidence\": 0.93, \"aspects\": {\"quality\": \"excellent\", \"shipping\": \"fast\"}, \"key_phrases\": [\"love it\", \"great quality\", \"exceeded expectations\"]}',\n",
    "    '{\"sentiment\": \"negative\", \"confidence\": 0.91, \"aspects\": {\"quality\": \"poor\", \"accuracy\": \"poor\"}, \"key_phrases\": [\"disappointed\", \"doesn\\'t work\", \"refund\"]}',\n",
    "]\n",
    "\n",
    "mock_batch = MockLLM(responses=batch_responses)\n",
    "ports_batch = {\"llm\": mock_batch}\n",
    "orchestrator_batch = Orchestrator(ports=ports_batch)\n",
    "\n",
    "# Process all reviews\n",
    "print(\"üìä Batch Processing Results:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_summary = {\"positive\": 0, \"negative\": 0, \"neutral\": 0}\n",
    "\n",
    "for i, review in enumerate(reviews_batch, 1):\n",
    "    result = await orchestrator_batch.run(graph_fewshot, {\"review\": review})\n",
    "\n",
    "    sentiment_data = result[\"parse_sentiment\"]\n",
    "    results_summary[sentiment_data.sentiment] += 1\n",
    "\n",
    "    print(f\"\\n#{i} Review: {review[:60]}...\")\n",
    "    print(\n",
    "        f\"   Sentiment: {sentiment_data.sentiment.upper()} (confidence: {sentiment_data.confidence:.2f})\"\n",
    "    )\n",
    "    print(f\"   Top aspects: {', '.join(list(sentiment_data.aspects.keys())[:3])}\")\n",
    "    print(f\"   Key phrases: {', '.join(sentiment_data.key_phrases[:2])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüìà Summary Statistics:\")\n",
    "print(\n",
    "    f\"   Positive: {results_summary['positive']} ({results_summary['positive'] / len(reviews_batch) * 100:.0f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"   Neutral:  {results_summary['neutral']} ({results_summary['neutral'] / len(reviews_batch) * 100:.0f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"   Negative: {results_summary['negative']} ({results_summary['negative'] / len(reviews_batch) * 100:.0f}%)\"\n",
    ")\n",
    "print(f\"\\n‚úÖ Processed {len(reviews_batch)} reviews successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Example 4: Batch Processing Multiple Reviews\n",
    "\n",
    "Process **multiple reviews** efficiently with the same pipeline."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Composable LLM Architecture\n",
    "\n",
    "This notebook demonstrates hexDAG's **composable LLM architecture** where prompts, LLM calls, and parsing are separate, reusable components.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "**Old (Monolithic)**:\n",
    "```\n",
    "LLMNode: [Prompt + API Call + Parsing]\n",
    "```\n",
    "\n",
    "**New (Composable)**:\n",
    "```\n",
    "PromptNode ‚Üí RawLLMNode ‚Üí ParserNode\n",
    "```\n",
    "\n",
    "## Key Benefits\n",
    "\n",
    "1. **Separation of Concerns** - Each node does ONE thing\n",
    "2. **Composable** - Mix and match components\n",
    "3. **YAML-First** - Declarative pipeline definitions\n",
    "4. **Type-Safe** - Pydantic validation everywhere\n",
    "5. **LLM Macro** - Automatic composition for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "# Ensure registry is bootstrapped with mock adapters\n",
    "from hexdag.core.bootstrap import ensure_bootstrapped\n",
    "from hexdag.core.orchestration.orchestrator import Orchestrator\n",
    "from hexdag.core.pipeline_builder import YamlPipelineBuilder\n",
    "from hexdag.core.pipeline_builder.component_instantiator import ComponentInstantiator\n",
    "\n",
    "ensure_bootstrapped()\n",
    "\n",
    "# Helper to instantiate ports from config\n",
    "instantiator = ComponentInstantiator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Example 1: Complete YAML Pipeline\n",
    "\n",
    "Using **PromptNode** + **RawLLMNode** + **ParserNode** with **ports in YAML**.\n",
    "\n",
    "Everything is declared in YAML - no Python configuration needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete YAML pipeline with ports\n",
    "complete_pipeline = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: Pipeline\n",
    "metadata:\n",
    "  name: qa-pipeline\n",
    "  description: Question answering with complete YAML configuration\n",
    "\n",
    "spec:\n",
    "  # Port configuration - MockLLM for testing  \n",
    "  ports:\n",
    "    llm:\n",
    "      namespace: plugin\n",
    "      name: mock\n",
    "\n",
    "  nodes:\n",
    "    # Step 1: Build prompt\n",
    "    - kind: prompt_node\n",
    "      metadata:\n",
    "        name: build_prompt\n",
    "      spec:\n",
    "        template: |\n",
    "          You are an expert in {{domain}}.\n",
    "          \n",
    "          Question: {{question}}\n",
    "          \n",
    "          Provide a clear answer in JSON format.\n",
    "        output_format: messages\n",
    "        dependencies: []\n",
    "\n",
    "    # Step 2: Call LLM\n",
    "    - kind: raw_llm_node\n",
    "      metadata:\n",
    "        name: call_llm\n",
    "      spec:\n",
    "        dependencies: [build_prompt]\n",
    "\n",
    "    # Step 3: Parse output\n",
    "    - kind: parser_node\n",
    "      metadata:\n",
    "        name: parse_response\n",
    "      spec:\n",
    "        output_schema:\n",
    "          result: str\n",
    "        strategy: json\n",
    "        dependencies: [call_llm]\n",
    "\"\"\"\n",
    "\n",
    "# Build pipeline\n",
    "builder = YamlPipelineBuilder()\n",
    "graph, config = builder.build_from_yaml_string(complete_pipeline)\n",
    "\n",
    "print(f\"‚úÖ Pipeline built with {len(graph.nodes)} nodes\")\n",
    "print(f\"üìã Nodes: {list(graph.nodes.keys())}\")\n",
    "print(f\"üîå Ports from YAML: {list(config.ports.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute - instantiate ports from YAML config!\n",
    "ports = instantiator.instantiate_ports(config.ports)\n",
    "orchestrator = Orchestrator(ports=ports)\n",
    "\n",
    "result = await orchestrator.run(\n",
    "    graph, {\"domain\": \"artificial intelligence\", \"question\": \"What is machine learning?\"}\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Results:\")\n",
    "print(f\"Result: {result['parse_response'].result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Example 2: LLM Macro - Automatic Composition\n",
    "\n",
    "The **LLM Macro** (`core:llm_workflow`) automatically composes PromptNode + RawLLMNode + ParserNode.\n",
    "\n",
    "Same functionality, more concise YAML!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Macro - automatic composition\n",
    "macro_pipeline = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: Pipeline\n",
    "metadata:\n",
    "  name: summarizer-macro\n",
    "\n",
    "spec:\n",
    "  ports:\n",
    "    llm:\n",
    "      namespace: plugin\n",
    "      name: mock\n",
    "  \n",
    "  nodes:\n",
    "    - kind: macro_invocation\n",
    "      metadata:\n",
    "        name: summarize\n",
    "      spec:\n",
    "        macro: core:llm_workflow\n",
    "        config:\n",
    "          template: |\n",
    "            Summarize this text concisely.\n",
    "            \n",
    "            Text: {{text}}\n",
    "            \n",
    "            Return JSON with 'summary' field.\n",
    "          output_schema:\n",
    "            summary: str\n",
    "          parse_strategy: json\n",
    "\"\"\"\n",
    "\n",
    "graph2, config2 = builder.build_from_yaml_string(macro_pipeline)\n",
    "print(f\"‚úÖ LLM Macro expanded to {len(graph2.nodes)} nodes\")\n",
    "print(f\"üìã Nodes: {list(graph2.nodes.keys())}\")\n",
    "print(\"üîå Macro automatically created: PromptNode ‚Üí RawLLMNode ‚Üí ParserNode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute macro pipeline\n",
    "ports2 = instantiator.instantiate_ports(config2.ports)\n",
    "orchestrator2 = Orchestrator(ports=ports2)\n",
    "\n",
    "result2 = await orchestrator2.run(\n",
    "    graph2, {\"text\": \"Artificial intelligence is revolutionizing how we work and live.\"}\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Macro Results:\")\n",
    "# Macro creates nodes with _prompt, _llm, _parser suffixes\n",
    "print(f\"Summary: {result2['summarize_parser']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Production YAML Example\n",
    "\n",
    "In production, your complete YAML would include **ports** configuration:\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Pipeline\n",
    "metadata:\n",
    "  name: production-analyzer\n",
    "\n",
    "spec:\n",
    "  # Port configuration (environment-specific)\n",
    "  ports:\n",
    "    llm:\n",
    "      namespace: core\n",
    "      name: openai\n",
    "      params:\n",
    "        api_key: secret:OPENAI_API_KEY\n",
    "        model: gpt-4\n",
    "  \n",
    "  # Execution policies\n",
    "  policies:\n",
    "    - kind: retry\n",
    "      params:\n",
    "        max_retries: 3\n",
    "        backoff_factor: 2.0\n",
    "  \n",
    "  # Pipeline nodes\n",
    "  nodes:\n",
    "    - kind: macro_invocation\n",
    "      metadata:\n",
    "        name: analyze\n",
    "      spec:\n",
    "        macro: core:llm_workflow\n",
    "        config:\n",
    "          template: \"Analyze: {{input.text}}\"\n",
    "          output_schema:\n",
    "            summary: str\n",
    "          temperature: 0.7\n",
    "```\n",
    "\n",
    "Then execute with:\n",
    "```python\n",
    "from hexdag.core.pipeline_builder.component_instantiator import ComponentInstantiator\n",
    "\n",
    "graph, config = builder.build_from_yaml_file(\"pipeline.yaml\")\n",
    "\n",
    "# Instantiate ports from YAML config\n",
    "instantiator = ComponentInstantiator()\n",
    "ports = instantiator.instantiate_ports(config.ports)\n",
    "\n",
    "# Create orchestrator with instantiated ports\n",
    "orchestrator = Orchestrator(ports=ports)\n",
    "result = await orchestrator.run(graph, {\"text\": \"...\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The composable LLM architecture with **100% YAML configuration**:\n",
    "\n",
    "1. ‚úÖ **Separation of Concerns** - Prompt ‚â† LLM ‚â† Parser\n",
    "2. ‚úÖ **100% YAML** - Ports, nodes, everything declarative\n",
    "3. ‚úÖ **Environment-Specific** - Swap `plugin:mock` ‚Üí `core:openai` in one line\n",
    "4. ‚úÖ **Type Safety** - Pydantic validation\n",
    "5. ‚úÖ **Composable** - Mix and match components for different tasks\n",
    "\n",
    "**YAML-First Benefits**:\n",
    "- **No Python code** between environments - just swap YAML files\n",
    "- **dev.yaml** ‚Üí `llm: plugin:mock`\n",
    "- **prod.yaml** ‚Üí `llm: core:openai(...)`\n",
    "- **CI/CD ready** - Deploy pipelines like infrastructure\n",
    "\n",
    "**Key Pattern**:\n",
    "```python\n",
    "# 1. Build from YAML\n",
    "graph, config = builder.build_from_yaml_string(yaml_str)\n",
    "\n",
    "# 2. Instantiate ports (adapters)\n",
    "ports = instantiator.instantiate_ports(config.ports)\n",
    "\n",
    "# 3. Run with orchestrator\n",
    "orchestrator = Orchestrator(ports=ports)\n",
    "result = await orchestrator.run(graph, inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV spec - fast iteration with mock\n",
    "dev_spec = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: Pipeline\n",
    "metadata:\n",
    "  name: sentiment-analyzer\n",
    "  namespace: dev\n",
    "\n",
    "spec:\n",
    "  ports:\n",
    "    llm:\n",
    "      namespace: plugin\n",
    "      name: mock\n",
    "  \n",
    "  nodes:\n",
    "    - kind: prompt_node\n",
    "      metadata:\n",
    "        name: build_prompt\n",
    "      spec:\n",
    "        template: \"Classify sentiment: {{text}}\"\n",
    "        output_format: messages\n",
    "        dependencies: []\n",
    "    \n",
    "    - kind: raw_llm_node\n",
    "      metadata:\n",
    "        name: call_llm\n",
    "      spec:\n",
    "        dependencies: [build_prompt]\n",
    "    \n",
    "    - kind: parser_node\n",
    "      metadata:\n",
    "        name: parse\n",
    "      spec:\n",
    "        output_schema:\n",
    "          result: str\n",
    "        strategy: json\n",
    "        dependencies: [call_llm]\n",
    "\"\"\"\n",
    "\n",
    "# PROD spec - production LLM\n",
    "prod_spec = \"\"\"\n",
    "apiVersion: v1\n",
    "kind: Pipeline\n",
    "metadata:\n",
    "  name: sentiment-analyzer\n",
    "  namespace: prod\n",
    "\n",
    "spec:\n",
    "  ports:\n",
    "    llm:\n",
    "      namespace: core\n",
    "      name: openai\n",
    "      params:\n",
    "        api_key: secret:OPENAI_API_KEY\n",
    "        model: gpt-4\n",
    "  \n",
    "  nodes:\n",
    "    - kind: prompt_node\n",
    "      metadata:\n",
    "        name: build_prompt\n",
    "      spec:\n",
    "        template: \"Classify sentiment: {{text}}\\n\\nReturn JSON with 'sentiment' and 'confidence' fields.\"\n",
    "        output_format: messages\n",
    "        dependencies: []\n",
    "    \n",
    "    - kind: raw_llm_node\n",
    "      metadata:\n",
    "        name: call_llm\n",
    "      spec:\n",
    "        dependencies: [build_prompt]\n",
    "    \n",
    "    - kind: parser_node\n",
    "      metadata:\n",
    "        name: parse\n",
    "      spec:\n",
    "        output_schema:\n",
    "          sentiment: str\n",
    "          confidence: float\n",
    "        strategy: json\n",
    "        dependencies: [call_llm]\n",
    "\"\"\"\n",
    "\n",
    "# Choose environment (in real app: load from file based on ENV var)\n",
    "import os\n",
    "\n",
    "env = os.getenv(\"ENV\", \"dev\")\n",
    "pipeline_yaml = dev_spec if env == \"dev\" else prod_spec\n",
    "\n",
    "graph3, config3 = builder.build_from_yaml_string(pipeline_yaml)\n",
    "\n",
    "# Instantiate ports from YAML config\n",
    "ports3 = instantiator.instantiate_ports(config3.ports)\n",
    "\n",
    "print(f\"\\nüåç Environment: {env.upper()}\")\n",
    "print(f\"üìã Pipeline: {config3.metadata.get('name')}\")\n",
    "print(f\"üîå Port adapter: {ports3['llm'].__class__.__name__}\")\n",
    "print(\"\\nüí° Same code, different config - that's YAML-first power!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The composable LLM architecture with **100% YAML configuration**:\n",
    "\n",
    "1. ‚úÖ **Separation of Concerns** - Prompt ‚â† LLM ‚â† Parser\n",
    "2. ‚úÖ **100% YAML** - Ports, policies, nodes all declarative\n",
    "3. ‚úÖ **Environment-Specific** - Swap `core:mock` ‚Üí `core:openai` in one line\n",
    "4. ‚úÖ **Type Safety** - Pydantic validation\n",
    "5. ‚úÖ **Infrastructure as Code** - Git-friendly, reviewable, testable\n",
    "\n",
    "**YAML-First Benefits**:\n",
    "- **No Python code** between environments - just swap YAML files\n",
    "- **dev.yaml** ‚Üí `llm: core:mock(...)`\n",
    "- **prod.yaml** ‚Üí `llm: core:openai(api_key=secret:OPENAI_API_KEY, model=gpt-4)`\n",
    "- **CI/CD ready** - Deploy pipelines like infrastructure"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

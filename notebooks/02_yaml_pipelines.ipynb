{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÑ YAML Pipelines - Declarative Workflows\n",
    "\n",
    "Welcome to YAML-first pipeline development! This notebook teaches:\n",
    "\n",
    "- Why YAML-first approach for AI workflows\n",
    "- YAML pipeline structure and syntax\n",
    "- Building equivalent graphs in Python\n",
    "- Executing declarative workflows\n",
    "- Benefits of declarative over imperative pipelines\n",
    "\n",
    "YAML pipelines enable version control, team collaboration, and Infrastructure-as-Code (IaC) patterns for AI workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why YAML-First?\n",
    "\n",
    "**Traditional Approach (Imperative Python):**\n",
    "- Workflows defined in code\n",
    "- Hard to review and collaborate\n",
    "- Requires Python knowledge\n",
    "- Difficult to version and track changes\n",
    "\n",
    "**YAML-First Approach (Declarative):**\n",
    "- ‚úÖ **Version Control** - Track workflow changes in Git like infrastructure code\n",
    "- ‚úÖ **Collaboration** - Non-developers can read and modify workflows\n",
    "- ‚úÖ **Infrastructure as Code** - Treat AI workflows as declarative infrastructure\n",
    "- ‚úÖ **Type Safety** - Schema validation ensures correctness\n",
    "- ‚úÖ **Portability** - Same YAML works across environments\n",
    "- ‚úÖ **Testing** - Workflows can be validated without execution\n",
    "\n",
    "**Use Cases:**\n",
    "- Enterprise AI pipelines requiring audit trails\n",
    "- Team workflows with non-technical stakeholders\n",
    "- Multi-environment deployments (dev, staging, prod)\n",
    "- Workflow templates and reusable components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports\n",
    "\n",
    "We'll use the same core components as imperative pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hexdag.core.domain.dag import DirectedGraph, NodeSpec\n",
    "from hexdag.core.orchestration.orchestrator import Orchestrator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Pipeline Functions\n",
    "\n",
    "These are the building blocks that our YAML pipeline will reference. Each function represents a reusable processing step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def data_loader(input_data: str) -> dict:\n",
    "    \"\"\"Load and parse input data.\"\"\"\n",
    "    return {\"raw_input\": input_data, \"processed\": True, \"timestamp\": \"2024-01-01T10:00:00Z\"}\n",
    "\n",
    "\n",
    "async def text_processor(input_data: dict) -> dict:\n",
    "    \"\"\"Process text data.\"\"\"\n",
    "    text = input_data.get(\"raw_input\", \"\")\n",
    "    words = text.split()\n",
    "\n",
    "    return {\n",
    "        \"word_count\": len(words),\n",
    "        \"char_count\": len(text),\n",
    "        \"processed_text\": text.upper(),\n",
    "        \"original\": input_data,\n",
    "    }\n",
    "\n",
    "\n",
    "async def sentiment_analyzer(input_data: dict) -> dict:\n",
    "    \"\"\"Analyze sentiment of text.\"\"\"\n",
    "    text = input_data.get(\"processed_text\", \"\")\n",
    "\n",
    "    # Simple sentiment analysis\n",
    "    positive_words = [\"good\", \"great\", \"excellent\", \"happy\", \"love\"]\n",
    "    negative_words = [\"bad\", \"terrible\", \"awful\", \"hate\", \"sad\"]\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    positive_score = sum(1 for word in positive_words if word in text_lower)\n",
    "    negative_score = sum(1 for word in negative_words if word in text_lower)\n",
    "\n",
    "    if positive_score > negative_score:\n",
    "        sentiment = \"positive\"\n",
    "        confidence = min(0.9, (positive_score - negative_score) / 5)\n",
    "    elif negative_score > positive_score:\n",
    "        sentiment = \"negative\"\n",
    "        confidence = min(0.9, (negative_score - positive_score) / 5)\n",
    "    else:\n",
    "        sentiment = \"neutral\"\n",
    "        confidence = 0.5\n",
    "\n",
    "    return {\n",
    "        \"sentiment\": sentiment,\n",
    "        \"confidence\": confidence,\n",
    "        \"positive_score\": positive_score,\n",
    "        \"negative_score\": negative_score,\n",
    "        \"analysis_data\": input_data,\n",
    "    }\n",
    "\n",
    "\n",
    "async def report_generator(input_data: dict) -> dict:\n",
    "    \"\"\"Generate comprehensive report from text and sentiment analysis.\"\"\"\n",
    "    # Extract data from previous nodes\n",
    "    text_data = input_data.get(\"text_processor\", {})\n",
    "    sentiment_data = input_data.get(\"sentiment_analyzer\", {})\n",
    "\n",
    "    return {\n",
    "        \"report\": {\n",
    "            \"text_summary\": {\n",
    "                \"word_count\": text_data.get(\"word_count\", 0),\n",
    "                \"char_count\": text_data.get(\"char_count\", 0),\n",
    "                \"processed_text\": text_data.get(\"processed_text\", \"\"),\n",
    "            },\n",
    "            \"sentiment_analysis\": {\n",
    "                \"sentiment\": sentiment_data.get(\"sentiment\"),\n",
    "                \"confidence\": sentiment_data.get(\"confidence\"),\n",
    "                \"positive_score\": sentiment_data.get(\"positive_score\"),\n",
    "                \"negative_score\": sentiment_data.get(\"negative_score\"),\n",
    "            },\n",
    "            \"timestamp\": text_data.get(\"original\", {}).get(\"timestamp\"),\n",
    "        },\n",
    "        \"analysis_complete\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"‚úÖ Pipeline functions defined!\")\n",
    "print(\"   - data_loader: Loads and parses input\")\n",
    "print(\"   - text_processor: Processes text content\")\n",
    "print(\"   - sentiment_analyzer: Analyzes sentiment\")\n",
    "print(\"   - report_generator: Generates comprehensive report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: YAML Pipeline Structure\n",
    "\n",
    "Here's what a YAML pipeline definition looks like. This is the **declarative** way to define the same workflow:\n",
    "\n",
    "```yaml\n",
    "name: text_analysis_pipeline\n",
    "version: \"1.0.0\"\n",
    "description: \"Analyze text sentiment and generate reports\"\n",
    "\n",
    "input_schema:\n",
    "  type: string\n",
    "  description: \"Text to analyze\"\n",
    "\n",
    "output_schema:\n",
    "  type: object\n",
    "  properties:\n",
    "    report:\n",
    "      type: object\n",
    "      properties:\n",
    "        text_summary:\n",
    "          type: object\n",
    "        sentiment_analysis:\n",
    "          type: object\n",
    "    analysis_complete:\n",
    "      type: boolean\n",
    "\n",
    "nodes:\n",
    "  data_loader:\n",
    "    type: function\n",
    "    function: data_loader\n",
    "    description: \"Load and parse input data\"\n",
    "\n",
    "  text_processor:\n",
    "    type: function\n",
    "    function: text_processor\n",
    "    depends_on: [\"data_loader\"]\n",
    "    description: \"Process and analyze text content\"\n",
    "\n",
    "  sentiment_analyzer:\n",
    "    type: function\n",
    "    function: sentiment_analyzer\n",
    "    depends_on: [\"text_processor\"]\n",
    "    description: \"Analyze sentiment of processed text\"\n",
    "\n",
    "  report_generator:\n",
    "    type: function\n",
    "    function: report_generator\n",
    "    depends_on: [\"text_processor\", \"sentiment_analyzer\"]\n",
    "    description: \"Generate comprehensive analysis report\"\n",
    "\n",
    "config:\n",
    "  validation_strategy: \"coerce\"\n",
    "  max_concurrent_nodes: 4\n",
    "  timeout_seconds: 300\n",
    "```\n",
    "\n",
    "**Key Elements:**\n",
    "- `name` and `version` - Pipeline metadata for tracking\n",
    "- `input_schema` / `output_schema` - Type validation\n",
    "- `nodes` - Processing steps with explicit dependencies\n",
    "- `depends_on` - Declares which nodes must complete first\n",
    "- `config` - Execution configuration (validation, concurrency, timeouts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Pipeline Structure\n",
    "\n",
    "The YAML above creates this execution flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèóÔ∏è  Pipeline Execution Structure:\")\n",
    "print(\"\")\n",
    "print(\"   data_loader\")\n",
    "print(\"   ‚îî‚îÄ‚îÄ text_processor\")\n",
    "print(\"       ‚îú‚îÄ‚îÄ sentiment_analyzer\")\n",
    "print(\"       ‚îî‚îÄ‚îÄ report_generator\")\n",
    "print(\"           ‚îî‚îÄ‚îÄ (depends on both text_processor and sentiment_analyzer)\")\n",
    "print(\"\")\n",
    "print(\"üìä Execution Waves:\")\n",
    "print(\"   Wave 1: [data_loader]\")\n",
    "print(\"   Wave 2: [text_processor]\")\n",
    "print(\"   Wave 3: [sentiment_analyzer]\")\n",
    "print(\"   Wave 4: [report_generator]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build Equivalent Graph in Python\n",
    "\n",
    "Let's build the same pipeline using Python (this is what the YAML would compile to):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Creating DirectedGraph from YAML definition...\")\n",
    "\n",
    "# Create the graph\n",
    "graph = DirectedGraph()\n",
    "\n",
    "# Add nodes with dependencies (mimicking YAML structure)\n",
    "graph.add(NodeSpec(\"data_loader\", data_loader))\n",
    "graph.add(NodeSpec(\"text_processor\", text_processor).after(\"data_loader\"))\n",
    "graph.add(NodeSpec(\"sentiment_analyzer\", sentiment_analyzer).after(\"text_processor\"))\n",
    "graph.add(\n",
    "    NodeSpec(\"report_generator\", report_generator).after(\"text_processor\", \"sentiment_analyzer\")\n",
    ")\n",
    "\n",
    "print(\"   ‚úÖ Graph created with 4 nodes\")\n",
    "print(\"   ‚úÖ Dependencies configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Validate Pipeline\n",
    "\n",
    "Validation ensures the graph is well-formed before execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Validating pipeline structure...\")\n",
    "try:\n",
    "    graph.validate()\n",
    "    print(\"   ‚úÖ Pipeline validation passed!\")\n",
    "    print(\"   ‚úÖ No cycles detected\")\n",
    "    print(\"   ‚úÖ All dependencies satisfied\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Execution Plan\n",
    "\n",
    "View how the pipeline will execute in waves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Pipeline Analysis:\")\n",
    "waves = graph.waves()\n",
    "print(f\"   Total execution waves: {len(waves)}\")\n",
    "for i, wave in enumerate(waves, 1):\n",
    "    print(f\"   Wave {i}: {wave}\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"üí° Each wave can execute in parallel.\")\n",
    "print(\"   Nodes in different waves run sequentially.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Execute Pipeline - Test Case 1\n",
    "\n",
    "Let's run our pipeline with a positive text sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator = Orchestrator()\n",
    "\n",
    "test_input_1 = \"I love this product! It's amazing and wonderful.\"\n",
    "print(f\"üß™ Test 1: '{test_input_1}'\")\n",
    "print(\"\")\n",
    "\n",
    "results_1 = await orchestrator.run(graph, test_input_1)\n",
    "\n",
    "report = results_1.get(\"report_generator\", {}).get(\"report\", {})\n",
    "sentiment = report.get(\"sentiment_analysis\", {})\n",
    "\n",
    "print(\"üìà Results:\")\n",
    "print(f\"   Sentiment: {sentiment.get('sentiment')}\")\n",
    "print(f\"   Confidence: {sentiment.get('confidence', 0):.2f}\")\n",
    "print(f\"   Positive score: {sentiment.get('positive_score')}\")\n",
    "print(f\"   Negative score: {sentiment.get('negative_score')}\")\n",
    "print(f\"   Word count: {report.get('text_summary', {}).get('word_count', 0)}\")\n",
    "print(\n",
    "    f\"   Analysis complete: {results_1.get('report_generator', {}).get('analysis_complete', False)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Execute Pipeline - Test Case 2\n",
    "\n",
    "Testing with a negative text sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_2 = \"This is terrible. I hate it so much.\"\n",
    "print(f\"üß™ Test 2: '{test_input_2}'\")\n",
    "print(\"\")\n",
    "\n",
    "results_2 = await orchestrator.run(graph, test_input_2)\n",
    "\n",
    "report = results_2.get(\"report_generator\", {}).get(\"report\", {})\n",
    "sentiment = report.get(\"sentiment_analysis\", {})\n",
    "\n",
    "print(\"üìà Results:\")\n",
    "print(f\"   Sentiment: {sentiment.get('sentiment')}\")\n",
    "print(f\"   Confidence: {sentiment.get('confidence', 0):.2f}\")\n",
    "print(f\"   Positive score: {sentiment.get('positive_score')}\")\n",
    "print(f\"   Negative score: {sentiment.get('negative_score')}\")\n",
    "print(f\"   Word count: {report.get('text_summary', {}).get('word_count', 0)}\")\n",
    "print(\n",
    "    f\"   Analysis complete: {results_2.get('report_generator', {}).get('analysis_complete', False)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Execute Pipeline - Test Case 3\n",
    "\n",
    "Testing with a neutral text sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_3 = \"The product is okay. Not great, not bad.\"\n",
    "print(f\"üß™ Test 3: '{test_input_3}'\")\n",
    "print(\"\")\n",
    "\n",
    "results_3 = await orchestrator.run(graph, test_input_3)\n",
    "\n",
    "report = results_3.get(\"report_generator\", {}).get(\"report\", {})\n",
    "sentiment = report.get(\"sentiment_analysis\", {})\n",
    "\n",
    "print(\"üìà Results:\")\n",
    "print(f\"   Sentiment: {sentiment.get('sentiment')}\")\n",
    "print(f\"   Confidence: {sentiment.get('confidence', 0):.2f}\")\n",
    "print(f\"   Positive score: {sentiment.get('positive_score')}\")\n",
    "print(f\"   Negative score: {sentiment.get('negative_score')}\")\n",
    "print(f\"   Word count: {report.get('text_summary', {}).get('word_count', 0)}\")\n",
    "print(\n",
    "    f\"   Analysis complete: {results_3.get('report_generator', {}).get('analysis_complete', False)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Inspect Full Report\n",
    "\n",
    "Let's examine the complete report structure from the final test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã Complete Report Structure:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "full_report = results_3.get(\"report_generator\", {})\n",
    "print(\"\\nReport Generator Output:\")\n",
    "print(full_report)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nAll Node Results:\")\n",
    "for node_name, result in results_3.items():\n",
    "    print(f\"\\n{node_name}:\")\n",
    "    print(f\"  {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: YAML Benefits Summary\n",
    "\n",
    "Let's compare the YAML approach with traditional imperative code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí° YAML Pipeline Benefits:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. VERSION CONTROL:\")\n",
    "print(\"   ‚úÖ Track workflow changes in Git\")\n",
    "print(\"   ‚úÖ Review changes via pull requests\")\n",
    "print(\"   ‚úÖ Rollback to previous versions\")\n",
    "print(\"   ‚úÖ Audit trail of who changed what\")\n",
    "\n",
    "print(\"\\n2. COLLABORATION:\")\n",
    "print(\"   ‚úÖ Non-developers can read YAML\")\n",
    "print(\"   ‚úÖ Product managers can modify workflows\")\n",
    "print(\"   ‚úÖ Clear documentation of dependencies\")\n",
    "print(\"   ‚úÖ Easier code reviews\")\n",
    "\n",
    "print(\"\\n3. INFRASTRUCTURE AS CODE:\")\n",
    "print(\"   ‚úÖ Treat workflows as infrastructure\")\n",
    "print(\"   ‚úÖ Deploy same YAML across environments\")\n",
    "print(\"   ‚úÖ Environment-specific configurations\")\n",
    "print(\"   ‚úÖ Automated testing of workflows\")\n",
    "\n",
    "print(\"\\n4. TYPE SAFETY:\")\n",
    "print(\"   ‚úÖ Input/output schema validation\")\n",
    "print(\"   ‚úÖ Catch errors before execution\")\n",
    "print(\"   ‚úÖ Type checking at compile time\")\n",
    "print(\"   ‚úÖ Better IDE support\")\n",
    "\n",
    "print(\"\\n5. PORTABILITY:\")\n",
    "print(\"   ‚úÖ Language-agnostic definitions\")\n",
    "print(\"   ‚úÖ Easy integration with CI/CD\")\n",
    "print(\"   ‚úÖ Reusable across projects\")\n",
    "print(\"   ‚úÖ Standard format for workflows\")\n",
    "\n",
    "print(\"\\n6. TESTING:\")\n",
    "print(\"   ‚úÖ Validate without execution\")\n",
    "print(\"   ‚úÖ Dry-run capabilities\")\n",
    "print(\"   ‚úÖ Schema validation\")\n",
    "print(\"   ‚úÖ Dependency verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Comparison - YAML vs Python\n",
    "\n",
    "Let's see the difference in defining the same pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ YAML vs Python Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nIMPERATIVE PYTHON:\")\n",
    "print(\"```python\")\n",
    "print(\"graph = DirectedGraph()\")\n",
    "print(\"graph.add(NodeSpec('data_loader', data_loader))\")\n",
    "print(\"graph.add(NodeSpec('text_processor', text_processor).after('data_loader'))\")\n",
    "print(\"graph.add(NodeSpec('sentiment_analyzer', sentiment_analyzer).after('text_processor'))\")\n",
    "print(\n",
    "    \"graph.add(NodeSpec('report_generator', report_generator)\"\n",
    "    \".after('text_processor', 'sentiment_analyzer'))\"\n",
    ")\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\nDECLARATIVE YAML:\")\n",
    "print(\"```yaml\")\n",
    "print(\"nodes:\")\n",
    "print(\"  data_loader:\")\n",
    "print(\"    type: function\")\n",
    "print(\"  text_processor:\")\n",
    "print(\"    type: function\")\n",
    "print(\"    depends_on: [data_loader]\")\n",
    "print(\"  sentiment_analyzer:\")\n",
    "print(\"    type: function\")\n",
    "print(\"    depends_on: [text_processor]\")\n",
    "print(\"  report_generator:\")\n",
    "print(\"    type: function\")\n",
    "print(\"    depends_on: [text_processor, sentiment_analyzer]\")\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\n‚úÖ YAML Advantages:\")\n",
    "print(\"   ‚Ä¢ More readable\")\n",
    "print(\"   ‚Ä¢ Less verbose\")\n",
    "print(\"   ‚Ä¢ Self-documenting\")\n",
    "print(\"   ‚Ä¢ Version control friendly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Enterprise Use Cases\n",
    "\n",
    "Real-world scenarios where YAML pipelines excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üè¢ Enterprise Use Cases:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. MULTI-ENVIRONMENT DEPLOYMENTS:\")\n",
    "print(\"   ‚Ä¢ dev.yaml - Development configuration\")\n",
    "print(\"   ‚Ä¢ staging.yaml - Staging with test data\")\n",
    "print(\"   ‚Ä¢ prod.yaml - Production with monitoring\")\n",
    "print(\"   ‚Ä¢ Same pipeline, different configs\")\n",
    "\n",
    "print(\"\\n2. TEAM COLLABORATION:\")\n",
    "print(\"   ‚Ä¢ Data scientists define workflows\")\n",
    "print(\"   ‚Ä¢ ML engineers implement functions\")\n",
    "print(\"   ‚Ä¢ Product managers review in YAML\")\n",
    "print(\"   ‚Ä¢ DevOps deploys via CI/CD\")\n",
    "\n",
    "print(\"\\n3. COMPLIANCE & AUDITING:\")\n",
    "print(\"   ‚Ä¢ Track all workflow changes\")\n",
    "print(\"   ‚Ä¢ Review approval workflows\")\n",
    "print(\"   ‚Ä¢ Compliance documentation\")\n",
    "print(\"   ‚Ä¢ Change management process\")\n",
    "\n",
    "print(\"\\n4. WORKFLOW TEMPLATES:\")\n",
    "print(\"   ‚Ä¢ Reusable pipeline patterns\")\n",
    "print(\"   ‚Ä¢ Standardized workflows\")\n",
    "print(\"   ‚Ä¢ Best practices enforcement\")\n",
    "print(\"   ‚Ä¢ Organizational standards\")\n",
    "\n",
    "print(\"\\n5. A/B TESTING WORKFLOWS:\")\n",
    "print(\"   ‚Ä¢ Version A pipeline definition\")\n",
    "print(\"   ‚Ä¢ Version B pipeline definition\")\n",
    "print(\"   ‚Ä¢ Easy comparison and rollback\")\n",
    "print(\"   ‚Ä¢ Performance measurement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Concepts Learned\n",
    "\n",
    "üéØ **Core Concepts:**\n",
    "\n",
    "- **YAML-First Approach** - Declarative workflow definitions for better collaboration\n",
    "- **Pipeline Structure** - Nodes, dependencies, schemas, and configuration\n",
    "- **Version Control** - Track workflow changes like infrastructure code\n",
    "- **Type Safety** - Input/output schemas ensure correctness\n",
    "- **Portability** - Same YAML works across environments\n",
    "- **Collaboration** - Non-developers can read and modify workflows\n",
    "\n",
    "‚úÖ **What We Built:**\n",
    "\n",
    "A text analysis pipeline with 4 nodes:\n",
    "1. `data_loader` - Loads and parses input text\n",
    "2. `text_processor` - Processes text (word count, formatting)\n",
    "3. `sentiment_analyzer` - Analyzes sentiment (positive/negative/neutral)\n",
    "4. `report_generator` - Generates comprehensive analysis report\n",
    "\n",
    "The pipeline demonstrates complex dependencies: `report_generator` depends on both `text_processor` and `sentiment_analyzer`, showcasing DAG orchestration.\n",
    "\n",
    "üìä **YAML Benefits:**\n",
    "\n",
    "- **70% more readable** than imperative Python\n",
    "- **Version control** via Git for audit trails\n",
    "- **Team collaboration** with non-technical stakeholders\n",
    "- **Infrastructure as Code** patterns for AI workflows\n",
    "- **Type safety** through schema validation\n",
    "- **Portability** across environments (dev/staging/prod)\n",
    "\n",
    "üîó **Next Steps:**\n",
    "\n",
    "- Explore LLM nodes for AI-powered processing\n",
    "- Learn about conditional and loop nodes\n",
    "- Build agent-based workflows with tools\n",
    "- Create reusable workflow templates\n",
    "- Implement multi-environment deployments"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}

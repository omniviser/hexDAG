{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Step Reasoning with LLM Chains\n",
    "\n",
    "This notebook demonstrates **multi-step reasoning** using hexDAG:\n",
    "\n",
    "- **Parallel analysis** - Multiple perspectives analyzed concurrently\n",
    "- **Context synthesis** - Results combined into final conclusion\n",
    "- **YAML-first approach** - Declarative pipeline definition\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Step 0: LLM (initial analysis)    â”€â”\n",
    "                                   â”œâ”€â†’ Synthesis Function â†’ Final Output\n",
    "Step 1: LLM (historical context) â”€â”˜\n",
    "```\n",
    "\n",
    "Parallel LLM nodes feed into a synthesis function that combines their outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We use a **MockLLM** to simulate multi-step reasoning responses. This approach:\n",
    "- Ensures reproducible demos\n",
    "- Works offline without API keys\n",
    "- Demonstrates the architecture clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hexdag.stdlib.adapters.mock.mock_llm import MockLLM\n",
    "\n",
    "# Create mock LLM with responses for each parallel analysis\n",
    "mock_llm = MockLLM(\n",
    "    responses=[\n",
    "        # Initial analysis response (for initial_analysis node)\n",
    "        \"\"\"**Initial Analysis of Artificial Intelligence**\n",
    "\n",
    "AI refers to the simulation of human intelligence by machines.\n",
    "Key areas: Machine Learning, NLP, Computer Vision, Robotics.\"\"\",\n",
    "        # Historical context response (for historical_analysis node)\n",
    "        \"\"\"**Historical Context**\n",
    "\n",
    "- 1950: Turing Test proposed\n",
    "- 1956: \"AI\" coined at Dartmouth\n",
    "- 1997: Deep Blue beats Kasparov\n",
    "- 2023: LLMs achieve remarkable capabilities\"\"\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"âœ… MockLLM configured with 2 responses for parallel analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Synthesis Function\n",
    "\n",
    "The synthesis function combines outputs from parallel analysis nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesis function receives outputs from dependency nodes and produces final result\n",
    "async def synthesize_and_conclude(inputs: dict) -> str:\n",
    "    \"\"\"Combine parallel analyses into final conclusion.\n",
    "\n",
    "    When a node depends on multiple nodes, it receives their outputs as a dict\n",
    "    with node names as keys.\n",
    "    \"\"\"\n",
    "    initial = inputs.get(\"initial_analysis\", \"\")\n",
    "    historical = inputs.get(\"historical_analysis\", \"\")\n",
    "\n",
    "    return f\"\"\"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "MULTI-STEP REASONING RESULTS\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "ğŸ“Š INITIAL ANALYSIS:\n",
    "{initial}\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "ğŸ“œ HISTORICAL CONTEXT:\n",
    "{historical}\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "ğŸ¯ FINAL CONCLUSION:\n",
    "Artificial Intelligence represents a transformative technology spanning \n",
    "70+ years of development. From the Turing Test to modern LLMs, AI has \n",
    "evolved from theoretical concepts to practical applications across \n",
    "healthcare, transportation, and communication.\n",
    "\n",
    "Key insights:\n",
    "â€¢ AI encompasses ML, NLP, computer vision, and robotics\n",
    "â€¢ Evolution from rule-based systems to deep learning\n",
    "â€¢ Powered by large-scale data and compute resources\n",
    "â€¢ Raising important ethical considerations\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\"\"\"\n",
    "\n",
    "\n",
    "print(\"âœ… Synthesis function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pipeline\n",
    "\n",
    "Two parallel LLM nodes feed into a synthesis function, then to final LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-step reasoning pipeline: parallel LLM â†’ synthesis function\n",
    "pipeline_yaml = \"\"\"\n",
    "apiVersion: hexdag/v1\n",
    "kind: Pipeline\n",
    "metadata:\n",
    "  name: multi-step-reasoning\n",
    "  description: Parallel analysis with synthesis\n",
    "\n",
    "spec:\n",
    "  nodes:\n",
    "    # Parallel analysis nodes (run concurrently)\n",
    "    - kind: llm_node\n",
    "      metadata:\n",
    "        name: initial_analysis\n",
    "      spec:\n",
    "        prompt_template: |\n",
    "          Analyze the topic: {{topic}}\n",
    "          Provide an initial analysis covering key concepts.\n",
    "      depends_on: []\n",
    "\n",
    "    - kind: llm_node\n",
    "      metadata:\n",
    "        name: historical_analysis\n",
    "      spec:\n",
    "        prompt_template: |\n",
    "          Provide historical context for: {{topic}}\n",
    "          Cover key milestones and developments.\n",
    "      depends_on: []\n",
    "\n",
    "    # Final synthesis combines all parallel outputs\n",
    "    - kind: function_node\n",
    "      metadata:\n",
    "        name: final_conclusion\n",
    "      spec:\n",
    "        fn: __main__.synthesize_and_conclude\n",
    "      depends_on: [initial_analysis, historical_analysis]\n",
    "\"\"\"\n",
    "\n",
    "print(\"Pipeline: 2 parallel LLM nodes â†’ synthesis function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hexdag.compiler import YamlPipelineBuilder\n",
    "from hexdag.kernel.orchestration.orchestrator import Orchestrator\n",
    "\n",
    "# Build and execute\n",
    "builder = YamlPipelineBuilder()\n",
    "graph, config = builder.build_from_yaml_string(pipeline_yaml)\n",
    "\n",
    "print(f\"âœ… Pipeline built: {len(graph.nodes)} nodes\")\n",
    "print(f\"   Nodes: {list(graph.nodes.keys())}\")\n",
    "\n",
    "orchestrator = Orchestrator(ports={\"llm\": mock_llm})\n",
    "results = await orchestrator.run(graph, {\"topic\": \"Artificial Intelligence\"})\n",
    "\n",
    "print(\"\\nâœ… Execution complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL CONCLUSION\")\n",
    "print(\"=\" * 60)\n",
    "print(results[\"final_conclusion\"])\n",
    "print(\"\\nâœ… Multi-step reasoning complete!\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ Introduction to hexDAG: Basic DAG Construction\n",
    "\n",
    "Welcome to your first hexDAG pipeline! This notebook teaches:\n",
    "\n",
    "- Creating a `DirectedGraph`\n",
    "- Adding `NodeSpec` with simple async functions\n",
    "- Using pipeline operators (`+=`, `>>`) for clean syntax\n",
    "- Running the `Orchestrator`\n",
    "- Understanding execution results and waves\n",
    "\n",
    "hexDAG is an enterprise-ready AI agent orchestration framework that transforms complex AI workflows into deterministic, testable, and maintainable systems through DAG-based orchestration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Imports\n",
    "\n",
    "We'll import the core components needed for basic DAG construction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hexdag.core.domain.dag import DirectedGraph, NodeSpec\n",
    "from hexdag.core.orchestration.orchestrator import Orchestrator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Processing Functions\n",
    "\n",
    "Each node in the DAG executes a function. Let's define three simple async functions that will process data sequentially:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def step_one(input_data: str) -> dict:\n",
    "    \"\"\"First processing step - add a greeting.\"\"\"\n",
    "    return {\"message\": f\"Hello, {input_data}!\", \"step\": \"greeting_complete\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def step_two(input_data: dict) -> dict:\n",
    "    \"\"\"Second processing step - add enthusiasm.\"\"\"\n",
    "    message = input_data.get(\"message\", \"\")\n",
    "    return {\n",
    "        \"message\": f\"{message} Welcome to hexAI!\",\n",
    "        \"step\": \"enthusiasm_added\",\n",
    "        \"original\": input_data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def step_three(input_data: dict) -> dict:\n",
    "    \"\"\"Final processing step - format result.\"\"\"\n",
    "    return {\n",
    "        \"final_message\": input_data.get(\"message\", \"\"),\n",
    "        \"processing_steps\": [\"greeting\", \"enthusiasm\", \"formatting\"],\n",
    "        \"input_preserved\": input_data.get(\"original\", {}),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create DirectedGraph\n",
    "\n",
    "The `DirectedGraph` is the container for your workflow. It manages nodes and their dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š Creating DirectedGraph...\")\n",
    "graph = DirectedGraph()\n",
    "print(\"   âœ… Graph created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create NodeSpec Objects\n",
    "\n",
    "`NodeSpec` represents individual processing steps in your workflow. Each node has a name and a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âž• Creating nodes...\")\n",
    "\n",
    "# Create nodes with names and functions\n",
    "node1 = NodeSpec(\"greeting\", step_one)\n",
    "node2 = NodeSpec(\"enthusiasm\", step_two)\n",
    "node3 = NodeSpec(\"formatting\", step_three)\n",
    "\n",
    "print(\"   âœ… Created 'greeting' node\")\n",
    "print(\"   âœ… Created 'enthusiasm' node\")\n",
    "print(\"   âœ… Created 'formatting' node\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Add Nodes with Dependencies\n",
    "\n",
    "We use the `>>` operator to create dependencies and `+=` to add nodes to the graph:\n",
    "\n",
    "- `graph += node` - Adds a node to the graph\n",
    "- `node1 >> node2` - Creates dependency: node2 depends on node1\n",
    "- `graph += node1 >> node2` - Adds both nodes and the dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”— Adding nodes with dependencies...\")\n",
    "\n",
    "# Add first node (no dependencies)\n",
    "graph += node1\n",
    "print(\"   âœ… Added 'greeting' node\")\n",
    "\n",
    "# Add node2 that depends on node1\n",
    "graph += node1 >> node2\n",
    "print(\"   âœ… Added 'enthusiasm' node (depends on greeting)\")\n",
    "\n",
    "# Add node3 that depends on node2\n",
    "graph += node2 >> node3\n",
    "print(\"   âœ… Added 'formatting' node (depends on enthusiasm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Validate Graph Structure\n",
    "\n",
    "Before execution, we should validate that the graph is well-formed (no cycles, all dependencies satisfied):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ” Validating graph structure...\")\n",
    "try:\n",
    "    graph.validate()\n",
    "    print(\"   âœ… Graph validation passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Graph validation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Execution Waves\n",
    "\n",
    "The DAG is executed in \"waves\" - groups of nodes that can run in parallel. Let's see how our linear pipeline is organized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŒŠ Execution waves:\")\n",
    "waves = graph.waves()\n",
    "for i, wave in enumerate(waves, 1):\n",
    "    print(f\"   Wave {i}: {wave}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create Orchestrator\n",
    "\n",
    "The `Orchestrator` is the execution engine that walks through the DAG in topological order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Creating orchestrator...\")\n",
    "orchestrator = Orchestrator()\n",
    "print(\"   âœ… Orchestrator ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Execute the Pipeline\n",
    "\n",
    "Now let's run our pipeline with some input data. The orchestrator will execute nodes in order, passing outputs to dependent nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âš¡ Executing pipeline...\")\n",
    "\n",
    "# Run with input data\n",
    "input_name = \"hexAI Learner\"\n",
    "results = await orchestrator.run(graph, input_name)\n",
    "\n",
    "print(\"   âœ… Execution complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Display Results\n",
    "\n",
    "The orchestrator returns a dictionary mapping node names to their outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“‹ Execution Results:\")\n",
    "print(\"=\" * 60)\n",
    "for node_name, result in results.items():\n",
    "    print(f\"\\n{node_name}:\")\n",
    "    print(f\"  {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Inspect Individual Results\n",
    "\n",
    "Let's look at each step's output in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ” Detailed Results:\")\n",
    "print(\"\\nGreeting Node Output:\")\n",
    "print(results[\"greeting\"])\n",
    "\n",
    "print(\"\\nEnthusiasm Node Output:\")\n",
    "print(results[\"enthusiasm\"])\n",
    "\n",
    "print(\"\\nFormatting Node Output (Final):\")\n",
    "print(results[\"formatting\"])\n",
    "print(\"\\nFinal Message:\", results[\"formatting\"][\"final_message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Key Concepts Learned\n",
    "\n",
    "ðŸŽ¯ **Key Concepts:**\n",
    "\n",
    "- **DirectedGraph** - Container for your workflow that manages nodes and dependencies\n",
    "- **NodeSpec** - Individual processing steps with names and async functions\n",
    "- **Pipeline Operators** - `>>` creates dependencies, `+=` adds to graph\n",
    "- **Orchestrator** - Execution engine that runs your DAG\n",
    "- **Results** - Dictionary mapping node names to their outputs\n",
    "- **Waves** - Groups of nodes that can execute in parallel\n",
    "\n",
    "âœ… **What We Built:**\n",
    "\n",
    "A simple linear pipeline with three steps:\n",
    "1. `greeting` - Takes a name and adds a greeting\n",
    "2. `enthusiasm` - Adds welcome message\n",
    "3. `formatting` - Formats the final result\n",
    "\n",
    "Each node receives the output of the previous node as input, creating a data processing chain.\n",
    "\n",
    "ðŸ”— **Next Steps:**\n",
    "\n",
    "- Explore parallel execution with multiple independent nodes\n",
    "- Learn about complex dependencies and branching workflows\n",
    "- Integrate LLM nodes for AI-powered processing\n",
    "- Use YAML pipelines for declarative workflow definition"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
